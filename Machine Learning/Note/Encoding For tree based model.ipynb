{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the note for how to do encoding with tree based model. Most of the example and plot is from this [article](https://towardsdatascience.com/one-hot-encoding-is-making-your-tree-based-ensembles-worse-heres-why-d64b282b5769).\n",
    "\n",
    "Tree based models like random forest do worse with one hot encoding. And the inner order of one feature is not necessary for label encoding when using tress based models.\n",
    "\n",
    "![Snapshot of Performance](https://cdn-images-1.medium.com/max/1200/1*vWhYH9KaUeDhjNB6cOU_sw.png)\n",
    "<center>(Snapshot of Performance)</center>\n",
    "\n",
    "### What happend inside the algorithm?\n",
    "For every tree based algorithm, there is a sub-algrithm that splits all the instance into two bins based on one feature. This algritm will consider all possible splits(based on all features and all possible values for each feature) and finds the most optimum split based on a criterion. And qualitatively speaking, the criterion helps the sub-algorithm select the split that minimizes the impurity of bins. \n",
    "So if a continuous variable is chosen for split, then there would be a number of choices of values on which tree can split and in most case, tree can grow in both directions.\n",
    "![](https://cdn-images-1.medium.com/max/1200/1*rJldhOQ8qofb-UsvEDwAdg.png)\n",
    "<center>Dense Decision Tree (Model without One Hot Encoding)</center>\n",
    "\n",
    "But it is totally different for categorical variables, it is naturally disadvantaged in this case and have only a few options for splitting which results in very sparse decision tree.\n",
    "The situation gets worse in variables that have small number of levels and one-hot encoding has only two values in each features.\n",
    "![](https://cdn-images-1.medium.com/max/1200/1*waMbIQifR03o_1hHzNYgbw.png)\n",
    "<center>Sparse Decision Tree (Model with One Hot Encoding)</center>\n",
    "Since the data set after onehot encoding become more sparse, the trees generally tend to grow in one direction: direction of majority, zeroes, in the dummy variables. \n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1600/1*jOMNT-nHwABGVchKX0Pi3Q.jpeg)\n",
    "\n",
    "If we have a categorical variable with q levels, the tree has to choose from ((2^q/2)-1) splits. For a dummy variable, there is only one possible split and this induces sparsity.\n",
    "In conclusion:\n",
    "1. The sparsity of dummy variables will lead trees tend to grow in one direction.\n",
    "2. The trees will be deeper. \n",
    "    * If we have a q levels categorical variable, we will have (2^q/2 -1) splits.\n",
    "    \n",
    "### Why it's harmful?\n",
    "\n",
    "\n",
    "1. By one-coding a feature into data set, we are inducing sparsity into the dataset and it is undesirable.\n",
    "2. The splitting algorithm treat all the dummy variables as independent vars, which is not true. And the gain of purity from make a split on a dummy variable is marginal(very slight). As a result, it's harder to let tree to select a dummy varriables closer to the root.(tree prefer a non-dummy one even the categorical variable is crucial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "Before and after we do one-hot encoding: (feature importance)\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1600/1*aMaOMQ0bIt9txo_YMcSiTQ.png)\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1600/1*VT1vwxH9k_Ra1B6JkMwSJw.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DYRK1A_N</th>\n",
       "      <th>ITSN1_N</th>\n",
       "      <th>BDNF_N</th>\n",
       "      <th>NR1_N</th>\n",
       "      <th>NR2A_N</th>\n",
       "      <th>pAKT_N</th>\n",
       "      <th>pBRAF_N</th>\n",
       "      <th>pCAMKII_N</th>\n",
       "      <th>pCREB_N</th>\n",
       "      <th>pELK_N</th>\n",
       "      <th>...</th>\n",
       "      <th>BAD_N</th>\n",
       "      <th>BCL2_N</th>\n",
       "      <th>pS6_N</th>\n",
       "      <th>pCFOS_N</th>\n",
       "      <th>SYP_N</th>\n",
       "      <th>H3AcK18_N</th>\n",
       "      <th>EGR1_N</th>\n",
       "      <th>H3MeK4_N</th>\n",
       "      <th>CaNA_N</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.503644</td>\n",
       "      <td>0.747193</td>\n",
       "      <td>0.430175</td>\n",
       "      <td>2.81633</td>\n",
       "      <td>5.99015</td>\n",
       "      <td>0.21883</td>\n",
       "      <td>0.177565</td>\n",
       "      <td>2.37374</td>\n",
       "      <td>0.232224</td>\n",
       "      <td>1.75094</td>\n",
       "      <td>...</td>\n",
       "      <td>0.122652</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.106305</td>\n",
       "      <td>0.108336</td>\n",
       "      <td>0.427099</td>\n",
       "      <td>0.114783</td>\n",
       "      <td>0.13179</td>\n",
       "      <td>0.128186</td>\n",
       "      <td>1.67565</td>\n",
       "      <td>c-CS-m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.514617</td>\n",
       "      <td>0.689064</td>\n",
       "      <td>0.41177</td>\n",
       "      <td>2.78951</td>\n",
       "      <td>5.68504</td>\n",
       "      <td>0.211636</td>\n",
       "      <td>0.172817</td>\n",
       "      <td>2.29215</td>\n",
       "      <td>0.226972</td>\n",
       "      <td>1.59638</td>\n",
       "      <td>...</td>\n",
       "      <td>0.116682</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.106592</td>\n",
       "      <td>0.104315</td>\n",
       "      <td>0.441581</td>\n",
       "      <td>0.111974</td>\n",
       "      <td>0.135103</td>\n",
       "      <td>0.131119</td>\n",
       "      <td>1.74361</td>\n",
       "      <td>c-CS-m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.509183</td>\n",
       "      <td>0.730247</td>\n",
       "      <td>0.418309</td>\n",
       "      <td>2.6872</td>\n",
       "      <td>5.62206</td>\n",
       "      <td>0.209011</td>\n",
       "      <td>0.175722</td>\n",
       "      <td>2.28334</td>\n",
       "      <td>0.230247</td>\n",
       "      <td>1.56132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.118508</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.108303</td>\n",
       "      <td>0.106219</td>\n",
       "      <td>0.435777</td>\n",
       "      <td>0.111883</td>\n",
       "      <td>0.133362</td>\n",
       "      <td>0.127431</td>\n",
       "      <td>1.92643</td>\n",
       "      <td>c-CS-m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.442107</td>\n",
       "      <td>0.617076</td>\n",
       "      <td>0.358626</td>\n",
       "      <td>2.46695</td>\n",
       "      <td>4.9795</td>\n",
       "      <td>0.222886</td>\n",
       "      <td>0.176463</td>\n",
       "      <td>2.1523</td>\n",
       "      <td>0.207004</td>\n",
       "      <td>1.59509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.132781</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.103184</td>\n",
       "      <td>0.111262</td>\n",
       "      <td>0.391691</td>\n",
       "      <td>0.130405</td>\n",
       "      <td>0.147444</td>\n",
       "      <td>0.146901</td>\n",
       "      <td>1.70056</td>\n",
       "      <td>c-CS-m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.43494</td>\n",
       "      <td>0.61743</td>\n",
       "      <td>0.358802</td>\n",
       "      <td>2.36578</td>\n",
       "      <td>4.71868</td>\n",
       "      <td>0.213106</td>\n",
       "      <td>0.173627</td>\n",
       "      <td>2.13401</td>\n",
       "      <td>0.192158</td>\n",
       "      <td>1.50423</td>\n",
       "      <td>...</td>\n",
       "      <td>0.129954</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.104784</td>\n",
       "      <td>0.110694</td>\n",
       "      <td>0.434154</td>\n",
       "      <td>0.118481</td>\n",
       "      <td>0.140314</td>\n",
       "      <td>0.14838</td>\n",
       "      <td>1.83973</td>\n",
       "      <td>c-CS-m</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   DYRK1A_N   ITSN1_N    BDNF_N    NR1_N   NR2A_N    pAKT_N   pBRAF_N  \\\n",
       "0  0.503644  0.747193  0.430175  2.81633  5.99015   0.21883  0.177565   \n",
       "1  0.514617  0.689064   0.41177  2.78951  5.68504  0.211636  0.172817   \n",
       "2  0.509183  0.730247  0.418309   2.6872  5.62206  0.209011  0.175722   \n",
       "3  0.442107  0.617076  0.358626  2.46695   4.9795  0.222886  0.176463   \n",
       "4   0.43494   0.61743  0.358802  2.36578  4.71868  0.213106  0.173627   \n",
       "\n",
       "  pCAMKII_N   pCREB_N   pELK_N  ...     BAD_N BCL2_N     pS6_N   pCFOS_N  \\\n",
       "0   2.37374  0.232224  1.75094  ...  0.122652    NaN  0.106305  0.108336   \n",
       "1   2.29215  0.226972  1.59638  ...  0.116682    NaN  0.106592  0.104315   \n",
       "2   2.28334  0.230247  1.56132  ...  0.118508    NaN  0.108303  0.106219   \n",
       "3    2.1523  0.207004  1.59509  ...  0.132781    NaN  0.103184  0.111262   \n",
       "4   2.13401  0.192158  1.50423  ...  0.129954    NaN  0.104784  0.110694   \n",
       "\n",
       "      SYP_N H3AcK18_N    EGR1_N  H3MeK4_N   CaNA_N  target  \n",
       "0  0.427099  0.114783   0.13179  0.128186  1.67565  c-CS-m  \n",
       "1  0.441581  0.111974  0.135103  0.131119  1.74361  c-CS-m  \n",
       "2  0.435777  0.111883  0.133362  0.127431  1.92643  c-CS-m  \n",
       "3  0.391691  0.130405  0.147444  0.146901  1.70056  c-CS-m  \n",
       "4  0.434154  0.118481  0.140314   0.14838  1.83973  c-CS-m  \n",
       "\n",
       "[5 rows x 78 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "data_class = fetch_openml(name='miceprotein', version=4)\n",
    "data = pd.DataFrame(data= np.c_[data_class['data'], data_class['target']],\n",
    "                     columns= data_class['feature_names'] + ['target'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding examples\n",
    "# For label encoding:\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(data['target'])\n",
    "data['target'] = le.transform(data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For one-hot encoding:\n",
    "data = pd.get_dummies(data, columns= ['target'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
