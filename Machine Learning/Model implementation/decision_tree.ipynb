{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree can be treated as a conditional probability distribution on the feature space. And it is a set of if-then rule. \n",
    "\n",
    "将决策树转换成if-then规则的过程如下：\n",
    "\n",
    "* 由决策树的根节点到叶节点的每一条路径构建一条规则；\n",
    "* 路径内部结点的特征对应规则的条件；(if)\n",
    "* 叶节点的类对应规则的结论.(then)\n",
    "\n",
    "且决策树的路径具有一个重要的性质：**互斥（mutually exclusive）且完备**,即每一个样本均被且只能被一条路径所覆盖。\n",
    "\n",
    "决策树学习算法主要由三部分构成：\n",
    "\n",
    "* 特征选择\n",
    "* 决策树生成\n",
    "* 决策树的剪枝\n",
    "\n",
    "### 1. Feature selection\n",
    "\n",
    "#### Gini impurity\n",
    "Used by the CART (classification and regression tree) algorithm for classification trees, Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset. \n",
    "\n",
    "#### Information gain\n",
    "\n",
    "Use entropy to evaluate the feature importance. \n",
    "\n",
    "Let $X$ be a discrete random variable with limited values, the distribution is:\n",
    "\n",
    "$P(X = x_i) = p_i , i = 1, 2, ..., n$\n",
    "\n",
    "And the entropy of $X$ is \n",
    "\n",
    "$H(X)= - \\sum_{i=1}^{n} p_{i}log(p_i)$.\n",
    "\n",
    "(The **intuition** that we have log(p) in entrop: It's binary search, the entropy is the expection of question we need for understanding the random result. [link](https://www.youtube.com/watch?v=9r7FIXEAGvs&list=PLs8w1Cdi-zvbEpRC8YqZcSIKipAe1QHGt))\n",
    "\n",
    "If X is 0, 1 r.v., if $P(X = 1) = p$, the entropy is:\n",
    "\n",
    "$H(X) = - p_{i}\\log(p_i) - (1 - p_{i})\\log(p_i)$\n",
    "\n",
    "Then we take the derivative: \n",
    "\n",
    "$\\frac{\\partial H(X)}{\\partial p} = -log(\\frac{p}{1-p})$\n",
    "\n",
    "![](9911d23ae3bcc854e59a59365b5365be_hd.png)\n",
    "\n",
    "So the information we gain after we know the feature could be:\n",
    "$g(D,A)=H(D)-H(D|A)$ \n",
    "\n",
    "and $H(Y|X)$ is called mutual information. So for the train set $D$, we should find the feature $A$ that give the greatest information gain $g(D, A)$.\n",
    "\n",
    "#### Compute the information gain (reduction in the randomness)\n",
    "$g(D,A)=H(D)-H(D|A)$ \n",
    "\n",
    "For feature $A$, we have $a_1, a_2, ..., a_n$ different value, we can divide $D$ to $D_1, D_2, ..., D_n$ by A.\n",
    "\n",
    "$H(D) = - \\sum_{k=1}^{K}\\frac{|C_k|}{|D|}log\\frac{|C_k|}{|D|}$\n",
    "\n",
    "Mutual information is just the weighted mean of entropy in $D_1, D_2, ..., D_n$.\n",
    "\n",
    "(**Example: if A is just the class label(most informative), then Each $D_i$ has just zero entropy(well classfied.)**)\n",
    "\n",
    "$H(D|A) = \\sum_{i=1}^{n}\\frac{|D_i|}{|D|}H(D_i) = \\sum_{i=1}^{n}\\frac{|D_i|}\n",
    "{|D|}\\sum_{k=1}^{K}\\frac{|D_{ik}|}{D_i}log\\frac{|D_{ik}|}{D_i}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5052408149441479"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data=[['slashdot','USA','yes',18,'None'],\n",
    "        ['google','France','yes',23,'Premium'],\n",
    "        ['digg','USA','yes',24,'Basic'],\n",
    "        ['kiwitobes','France','yes',23,'Basic'],\n",
    "        ['google','UK','no',21,'Premium'],\n",
    "        ['(direct)','New Zealand','no',12,'None'],\n",
    "        ['(direct)','UK','no',21,'Basic'],\n",
    "        ['google','USA','no',24,'Premium'],\n",
    "        ['slashdot','France','yes',19,'None'],\n",
    "        ['digg','USA','no',18,'None'],\n",
    "        ['google','UK','no',18,'None'],\n",
    "        ['kiwitobes','UK','no',19,'None'],\n",
    "        ['digg','New Zealand','yes',12,'Basic'],\n",
    "        ['slashdot','UK','no',21,'None'],\n",
    "        ['google','UK','yes',18,'Basic'],\n",
    "        ['kiwitobes','France','yes',19,'Basic']]\n",
    "entropy(my_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算各个y的数量\n",
    "def uniquecounts(rows):\n",
    "    results = {}\n",
    "    for row in rows:\n",
    "        # we put the count in the last row\n",
    "        r = row[-1]\n",
    "        if r not in results:\n",
    "            results[r] = 0\n",
    "        results[r] += 1\n",
    "    return results\n",
    "\n",
    "def entropy(rows):\n",
    "    from math import log\n",
    "    log2 = lambda x: log(x)/log(2)\n",
    "    results = uniquecounts(rows)\n",
    "    ent = 0\n",
    "    for r in results.keys():\n",
    "        p = float(results[r])/len(rows)\n",
    "        ent -= p * log2(p)\n",
    "    return ent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ID3算法\n",
    "    * select the feature with largest information gain, say A.\n",
    "    * assign A as a decision attribute for Node.\n",
    "    * for each value of A, create a desecendent of Node\n",
    "    * sort train examples to leaves\n",
    "    * recursion until the examples are well classfied.\n",
    "2. C4.5\n",
    "    * use gain ratio rather than information gain.\n",
    "    * $ratio(D, A) = \\frac{g(D, A)}{H_A(D)} $\n",
    "        * where $H_A(D) = -\\sum^n_{i=1}\\frac{|D_i|}{D}\\log_2\\frac{|D_i|}{D}$ (n is the feature #values)\n",
    "    * **we prefer the feature with less valus**\n",
    "3. CART\n",
    "    * binary tree\n",
    "    * gini impurity,假设有K个类别，每个类别的概率为$p_k$. (biggest if p_k is even and K is big.)\n",
    "        * $Gini(p)=\\sum_{k=1}^{K}p_{k}(1-p_k)=1-\\sum_{k=1}^{K}p_{k}^{2}$\n",
    "        \n",
    "        ![](a5f21f511372fea308ab5a2877958e77_hd.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义节点的属性\n",
    "class decisionnode:\n",
    "    def __init__(self,col = -1,value = None, results = None, tb = None,fb = None):\n",
    "        self.col = col   # col是待检验的判断条件所对应的列索引值\n",
    "        self.value = value # value对应于为了使结果为True，当前列必须匹配的值\n",
    "        self.results = results #保存的是针对当前分支的结果，它是一个字典\n",
    "        self.tb = tb ## desision node,对应于结果为true时，树上相对于当前节点的子树上的节点\n",
    "        self.fb = fb ## desision node,对应于结果为false时，树上相对于当前节点的子树上的节点\n",
    "\n",
    "\n",
    "\n",
    "# 基尼不纯度\n",
    "\n",
    "\n",
    "# 随机放置的数据项出现于错误分类中的概率\n",
    "def giniimpurity(rows):\n",
    "    total = len(rows)\n",
    "    counts = uniquecounts(rows)\n",
    "    imp =0\n",
    "    for k1 in counts:\n",
    "        p1 = float(counts[k1])/total\n",
    "        for k2 in counts: # 这个循环是否可以用（1-p1）替换？\n",
    "            if k1 == k2: continue\n",
    "            p2 = float(counts[k2])/total\n",
    "            imp+=p1*p2\n",
    "    return imp\n",
    "\n",
    "\n",
    "# 改进giniimpurity\n",
    "\n",
    "\n",
    "def giniimpurity_2(rows):\n",
    "    total = len(rows)\n",
    "    counts = uniquecounts(rows)\n",
    "    imp = 0\n",
    "    for k1 in counts.keys():\n",
    "        p1 = float(counts[k1])/total\n",
    "        imp+= p1*(1-p1)\n",
    "    return imp\n",
    "\n",
    "\n",
    "\n",
    "#在某一列上对数据集进行拆分。可应用于数值型或因子型变量\n",
    "def divideset(rows,column,value):\n",
    "    #定义一个函数，判断当前数据行属于第一组还是第二组\n",
    "    split_function = None\n",
    "    if isinstance(value,int) or isinstance(value,float):\n",
    "        split_function = lambda row:row[column] >= value\n",
    "    else:\n",
    "        split_function = lambda row:row[column]==value\n",
    "    # 将数据集拆分成两个集合，并返回\n",
    "    set1 = [row for row in rows if split_function(row)]\n",
    "    set2 = [row for row in rows if not split_function(row)]\n",
    "    return(set1,set2)\n",
    "\n",
    "\n",
    "# 以递归方式构造树\n",
    "\n",
    "def buildtree(rows,scoref = entropy):\n",
    "    if len(rows)==0 : return decisionnode()\n",
    "    current_score = scoref(rows)\n",
    "    \n",
    "    # 定义一些变量以记录最佳拆分条件\n",
    "    best_gain = 0.0\n",
    "    best_criteria = None\n",
    "    best_sets = None\n",
    "    \n",
    "    column_count = len(rows[0]) - 1\n",
    "    for col in range(0,column_count):\n",
    "        #在当前列中生成一个由不同值构成的序列\n",
    "        column_values = {}\n",
    "        for row in rows:\n",
    "            column_values[row[col]] = 1 # 初始化\n",
    "        #根据这一列中的每个值，尝试对数据集进行拆分\n",
    "        for value in column_values.keys():\n",
    "            (set1,set2) = divideset(rows,col,value)\n",
    "            \n",
    "            # 信息增益\n",
    "            p = float(len(set1))/len(rows)\n",
    "            gain = current_score - p*scoref(set1) - (1-p)*scoref(set2)\n",
    "            if gain>best_gain and len(set1)>0 and len(set2)>0:\n",
    "                best_gain = gain\n",
    "                best_criteria = (col,value)\n",
    "                best_sets = (set1,set2)\n",
    "                \n",
    "    #创建子分支\n",
    "    if best_gain>0:\n",
    "        trueBranch = buildtree(best_sets[0])  #递归调用\n",
    "        falseBranch = buildtree(best_sets[1])\n",
    "        return decisionnode(col = best_criteria[0],value = best_criteria[1],\n",
    "                            tb = trueBranch,fb = falseBranch)\n",
    "    else:\n",
    "        return decisionnode(results = uniquecounts(rows))\n",
    "\n",
    "# 决策树的显示\n",
    "def printtree(tree,indent = ''):\n",
    "    # 是否是叶节点\n",
    "    if tree.results!=None:\n",
    "        print str(tree.results)\n",
    "    else:\n",
    "        # 打印判断条件\n",
    "        print str(tree.col)+\":\"+str(tree.value)+\"? \"\n",
    "        #打印分支\n",
    "        print indent+\"T->\",\n",
    "        printtree(tree.tb,indent+\" \")\n",
    "        print indent+\"F->\",\n",
    "        printtree(tree.fb,indent+\" \")\n",
    "\n",
    "\n",
    "# 对新的观测数据进行分类\n",
    "\n",
    "\n",
    "def classify(observation,tree):\n",
    "    if tree.results!= None:\n",
    "        return tree.results\n",
    "    else:\n",
    "        v = observation[tree.col]\n",
    "        branch = None\n",
    "        if isinstance(v,int) or isinstance(v,float):\n",
    "            if v>= tree.value: branch = tree.tb\n",
    "            else: branch = tree.fb\n",
    "        else:\n",
    "            if v==tree.value : branch = tree.tb\n",
    "            else: branch = tree.fb\n",
    "        return classify(observation,branch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
